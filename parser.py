import asyncio
import httpx
import re
import base64
import json
import os
import ipaddress
import math
from urllib.parse import urlparse

SOURCES = [
    #"https://raw.githubusercontent.com/Epodonios/v2ray-configs/refs/heads/main/All_Configs_Sub.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Config/refs/heads/main/All_Configs_Sub.txt",
    #"https://raw.githubusercontent.com/ebrasha/free-v2ray-public-list/refs/heads/main/all_extracted_configs.txt"
]

TIMEOUT = 7.0
CONCURRENT_LIMIT = 50
SERVERS_PER_FILE = 200  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ—Ä–≤–µ—Ä–æ–≤ –≤ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ

ALLOWED_PROTOCOLS = ['vless', 'vmess', 'ss']  # –¢–æ–ª—å–∫–æ —ç—Ç–∏ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã


class TurboParser:
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

    def decode_base64(self, text):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ Base64"""
        try:
            text = re.sub(r'\s+', '', text)
            missing_padding = len(text) % 4
            if missing_padding:
                text += '=' * (4 - missing_padding)
            decoded = base64.b64decode(text).decode('utf-8', errors='ignore')
            return decoded
        except Exception as e:
            return ""

    def extract_keys(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return []

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ (–≤–∫–ª—é—á–∞—è trojan, –Ω–æ –º—ã –µ–≥–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ–∑–∂–µ)
        patterns = [
            r'(vmess://[a-zA-Z0-9+/=]+)',
            r'(vless://[a-f0-9-]+@[a-zA-Z0-9.-]+:\d+)',
            r'(ss://[a-zA-Z0-9+/=]+[@#])',
            r'(trojan://[a-zA-Z0-9-]+@[a-zA-Z0-9.-]+:\d+)',
            r'(ss://[a-zA-Z0-9+/=]+)',
        ]

        found = []
        for pattern in patterns:
            found.extend(re.findall(pattern, text, re.IGNORECASE))

        return list(set(found))

    def filter_by_protocol(self, configs):
        """
        –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ —Å–µ—Ä–≤–µ—Ä—ã —Å —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–º–∏ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º–∏
        """
        filtered = []
        removed = []

        for config in configs:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª
            protocol = config.split('://')[0].lower()

            if protocol in ALLOWED_PROTOCOLS:
                filtered.append(config)
            else:
                removed.append(config)

        print(f" –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤:")
        print(f"    –û—Å—Ç–∞–≤–ª–µ–Ω–æ ({len(filtered)}): {', '.join(ALLOWED_PROTOCOLS)}")
        print(f"    –£–¥–∞–ª–µ–Ω–æ ({len(removed)}): trojan –∏ –¥—Ä—É–≥–∏–µ")

        return filtered

    def extract_host_port(self, config):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–æ—Å—Ç–∞ –∏ –ø–æ—Ä—Ç–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞"""
        try:
            if config.startswith('vmess://'):
                try:
                    vmess_data = config[8:]
                    decoded = self.decode_base64(vmess_data)
                    if decoded:
                        data = json.loads(decoded)
                        return data.get('add'), int(data.get('port', 0))
                except:
                    pass

            parsed = urlparse(config)
            if parsed.hostname and parsed.port:
                return parsed.hostname, parsed.port

            parts = config.split('://')[1].split('@')
            if len(parts) > 1:
                addr_part = parts[-1]
            else:
                addr_part = parts[0]

            addr_part = addr_part.split('/')[0].split('?')[0]
            if ':' in addr_part:
                host, port_str = addr_part.split(':')
                return host, int(port_str)

        except Exception as e:
            pass
        return None, None

    async def fetch_source(self, client, url):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        try:
            print(f"[*] –ó–∞–ø—Ä–æ—Å –∫: {url}")
            r = await client.get(url, timeout=20.0, follow_redirects=True)
            print(f"    [!] –°—Ç–∞—Ç—É—Å: {r.status_code}, –†–∞–∑–º–µ—Ä: {len(r.text)} –±–∞–π—Ç")

            if r.status_code != 200:
                return []

            raw_data = r.text
            found = []

            found.extend(self.extract_keys(raw_data))

            for line in raw_data.split('\n'):
                line = line.strip()
                if line and len(line) > 20:
                    decoded = self.decode_base64(line)
                    if decoded and '://' in decoded:
                        found.extend(self.extract_keys(decoded))

            if found:
                print(f"    [+] –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(set(found))}")
            return list(set(found))
        except Exception as e:
            print(f"    [X] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}")
            return []

    async def check_server(self, config, semaphore):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–µ—Ä–∞"""
        host, port = self.extract_host_port(config)

        if not host or not port:
            return None

        async with semaphore:
            try:
                try:
                    ipaddress.ip_address(host)
                except ValueError:
                    try:
                        await asyncio.get_event_loop().getaddrinfo(host, port)
                    except:
                        return None

                conn = asyncio.open_connection(host, port)
                _, writer = await asyncio.wait_for(conn, timeout=TIMEOUT)
                writer.close()
                await writer.wait_closed()
                print(f"    [LIVE] {host}:{port}")
                return config
            except:
                return None


def split_into_files(data, base_filename="sub", items_per_file=SERVERS_PER_FILE):

    if not data:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏")
        return []

    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤
    subs_dir = os.path.join('deploy', 'subscriptions')
    os.makedirs(subs_dir, exist_ok=True)

    total_items = len(data)
    num_files = math.ceil(total_items / items_per_file)

    print(f"\n--- –†–ê–ó–ë–ò–í–ö–ê –ù–ê {num_files} –ú–ê–õ–ï–ù–¨–ö–ò–• –§–ê–ô–õ–û–í (–ø–æ ~{items_per_file} —Å–µ—Ä–≤–µ—Ä–æ–≤) ---")

    created_files = []

    for i in range(num_files):
        start_idx = i * items_per_file
        end_idx = min((i + 1) * items_per_file, total_items)

        # –¢–µ–∫—É—â–∏–π –∫—É—Å–æ–∫ —Å–µ—Ä–≤–µ—Ä–æ–≤
        chunk = data[start_idx:end_idx]
        chunk_text = "\n".join(chunk)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ —Å –≤–µ–¥—É—â–∏–º–∏ –Ω—É–ª—è–º–∏ (001, 002, ...)
        file_number = i + 1
        file_prefix = f"{base_filename}_{file_number:03d}"

        # 1. –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å —Å—ã—Ä—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
        txt_filename = f"{file_prefix}.txt"
        txt_path = os.path.join(subs_dir, txt_filename)
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(chunk_text)
        created_files.append(txt_path)

        # 2. Base64 —Ñ–∞–π–ª –¥–ª—è V2Ray/V2Box
        b64_filename = f"{file_prefix}_b64.txt"
        b64_path = os.path.join(subs_dir, b64_filename)
        chunk_b64 = base64.b64encode(chunk_text.encode()).decode()
        with open(b64_path, 'w', encoding='utf-8') as f:
            f.write(chunk_b64)
        created_files.append(b64_path)

        print(f"  [{file_number:03d}/{num_files:03d}] {txt_filename}: {len(chunk)} —Å–µ—Ä–≤–µ—Ä–æ–≤")


    # –°–æ–∑–¥–∞—ë–º —Ñ–∞–π–ª —Å–æ –≤—Å–µ–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
    create_links_file(subs_dir, num_files, base_filename)

    print(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(created_files)}")
    return created_files
    create_readme(subs_dir, num_files, items_per_file, total_items)
    
def create_readme(subs_dir, num_files, items_per_file, total_items):
    """–°–æ–∑–¥–∞—ë—Ç README –≤ –ø–∞–ø–∫–µ —Å –º–∞–ª–µ–Ω—å–∫–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏"""
    readme_path = os.path.join(subs_dir, 'README.md')
    
    content = f"""#  –ú–∞–ª–µ–Ω—å–∫–∏–µ –ø–æ–¥–ø–∏—Å–∫–∏ –ø–æ {items_per_file} —Å–µ—Ä–≤–µ—Ä–æ–≤

## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- **–í—Å–µ–≥–æ —Å–µ—Ä–≤–µ—Ä–æ–≤:** {total_items}
- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤:** {num_files}
- **–°–µ—Ä–≤–µ—Ä–æ–≤ –≤ —Ñ–∞–π–ª–µ:** ~{items_per_file}
- **–ü—Ä–æ—Ç–æ–∫–æ–ª—ã:** VLESS, VMess, SS
- **–§–æ—Ä–º–∞—Ç—ã:** –¢–µ–∫—Å—Ç (.txt) –∏ Base64 (_b64.txt)

##  –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏

"""
    
    base_url = "https://raw.githubusercontent.com/hiztin/VLESS-PO-GRIBI/main/deploy/subscriptions"
    
    for i in range(num_files):
        file_number = i + 1
        content += f"- **–ß–∞—Å—Ç—å {file_number:02d}**: [`sub_{file_number:03d}.txt`]({base_url}/sub_{file_number:03d}.txt) | [`sub_{file_number:03d}_b64.txt`]({base_url}/sub_{file_number:03d}_b64.txt)\n"
    
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"üìñ –°–æ–∑–¥–∞–Ω README: {readme_path}")


def create_links_file(subs_dir, num_files, base_filename):
    """–°–æ–∑–¥–∞—ë—Ç —Ñ–∞–π–ª —Å–æ –≤—Å–µ–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è"""
    links_path = os.path.join(subs_dir, 'all_links.txt')

    base_url = "https://raw.githubusercontent.com/hiztin/VLESS-PO-GRIBI/main/deploy/subscriptions"

    with open(links_path, 'w', encoding='utf-8') as f:
        f.write("# –í—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –º–∞–ª–µ–Ω—å–∫–∏–µ –ø–æ–¥–ø–∏—Å–∫–∏ (VLESS/VMess/SS)\n\n")
        f.write("## Base64 —Å—Å—ã–ª–∫–∏ (–¥–ª—è V2Ray/V2Box)\n")
        for i in range(num_files):
            file_num = i + 1
            f.write(f"{base_url}/sub_{file_num:03d}_b64.txt\n")

        f.write("\n## –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ (–¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞)\n")
        for i in range(num_files):
            file_num = i + 1
            f.write(f"{base_url}/sub_{file_num:03d}.txt\n")

    print(f"üîó –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª —Å–æ —Å—Å—ã–ª–∫–∞–º–∏: {links_path}")


def save_main_files(alive_servers, total_found):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã (–ø–æ–ª–Ω—ã–µ –ø–æ–¥–ø–∏—Å–∫–∏)"""
    os.makedirs('deploy', exist_ok=True)

    # –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å–æ –≤—Å–µ–º–∏ —Å–µ—Ä–≤–µ—Ä–∞–º–∏
    with open('deploy/sub.txt', 'w', encoding='utf-8') as f:
        f.write("\n".join(alive_servers))

    # Base64 —Ñ–∞–π–ª —Å–æ –≤—Å–µ–º–∏ —Å–µ—Ä–≤–µ—Ä–∞–º–∏
    with open('deploy/sub_base64.txt', 'w', encoding='utf-8') as f:
        all_b64 = base64.b64encode("\n".join(alive_servers).encode()).decode()
        f.write(all_b64)
        
    with open('deploy/debug.json', 'w', encoding='utf-8') as f:
        protocols = {}
        for server in alive_servers[:100]:  
            proto = server.split('://')[0]
            protocols[proto] = protocols.get(proto, 0) + 1

        json.dump({
            'total': total_found,
            'alive': len(alive_servers),
            'allowed_protocols': ALLOWED_PROTOCOLS,
            'date': str(__import__('datetime').datetime.now()),
            'protocol_stats': protocols,
            'servers_preview': alive_servers[:10]
        }, f, indent=2, ensure_ascii=False)

    print(f" –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ deploy/")
    print(f"   - sub.txt: {len(alive_servers)} —Å–µ—Ä–≤–µ—Ä–æ–≤ (VLESS/VMess/SS)")
    print(f"   - sub_base64.txt: –¥–ª—è V2Ray")
    print(f"   - debug.json: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")


def save_protocol_stats(servers):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª"""
    stats = {}
    for server in servers:
        protocol = server.split('://')[0]
        stats[protocol] = stats.get(protocol, 0) + 1

    stats_path = os.path.join('deploy', 'protocol_stats.json')
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump({
            'total': len(servers),
            'by_protocol': stats,
            'filtered_out': ['trojan']  
        }, f, indent=2)

    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º: {stats}")


async def main():
    parser = TurboParser()

    transport = httpx.AsyncHTTPTransport(retries=2)

    async with httpx.AsyncClient(
            transport=transport,
            verify=False,
            follow_redirects=True,
            headers=parser.headers,
            timeout=30.0
    ) as client:

        print("–°–ë–û–†")
        tasks = [parser.fetch_source(client, url) for url in SOURCES]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        raw_links = []
        for result in results:
            if isinstance(result, list):
                raw_links.extend(result)

        unique_links = list(set(raw_links))

        print(f"\n–ò–¢–û–ì–û –£–ù–ò–ö–ê–õ–¨–ù–´–•: {len(unique_links)}")

        if not unique_links:
            print(" –°—Å—ã–ª–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –°–æ–∑–¥–∞—é —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã")
            test_servers = [
                "vmess://eyJhZGQiOiJ0ZXN0LmNvbSIsInBvcnQiOiI4MCIsImlkIjoiMTIzNDU2Nzg5MCJ9",
                "vless://test-uuid@test.com:443?security=tls",
                "ss://YWVzLTI1Ni1nY206dGVzdEB0ZXN0LmNvbTo4MA=="
            ]
            save_main_files(test_servers, 3)
            split_into_files(test_servers, items_per_file=2)
            return

        print("\n2: –í–ê–õ–ò–î–ê–¶–ò–Ø")
        sem = asyncio.Semaphore(CONCURRENT_LIMIT)
        check_tasks = [parser.check_server(link, sem) for link in unique_links]
        valid_results = await asyncio.gather(*check_tasks)

        alive = [r for r in valid_results if r]

        print(f"\n 3: –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–û –ü–†–û–¢–û–ö–û–õ–ê–ú")
        filtered_servers = parser.filter_by_protocol(alive)

        print(f"\n--- –®–ê–ì 4: –°–û–•–†–ê–ù–ï–ù–ò–ï ({len(filtered_servers)} –∂–∏–≤—ã—Ö –∏–∑ {len(alive)} –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏) ---")

        if filtered_servers:
            save_main_files(filtered_servers, len(unique_links))


            save_protocol_stats(filtered_servers)
            split_into_files(filtered_servers, items_per_file=SERVERS_PER_FILE)

            print("\n" + "=" * 50)
            print(" –í–°–Å –ì–û–¢–û–í–û!")
            print("=" * 50)
            print(f" –í—Å–µ–≥–æ —Ä–∞–±–æ—á–∏—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤: {len(filtered_servers)}")
            print(f" –†–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã: {', '.join(ALLOWED_PROTOCOLS)}")
            print(f" –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: trojan –∏ –¥—Ä—É–≥–∏–µ")
            print(f" –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã: deploy/sub.txt, deploy/sub_base64.txt")
            print(f" –ú–∞–ª–µ–Ω—å–∫–∏–µ —Ñ–∞–π–ª—ã: deploy/subscriptions/ (–ø–∞–ø–∫–∞)")
            print("\n –°—Å—ã–ª–∫–∏ –¥–ª—è V2Ray/V2Box:")
            print(f"   ‚Ä¢ –ü–æ–ª–Ω–∞—è: https://raw.githubusercontent.com/hiztin/VLESS-PO-GRIBI/main/deploy/sub_base64.txt")
            print(f"   ‚Ä¢ –ü–æ —á–∞—Å—Ç—è–º: –≤ –ø–∞–ø–∫–µ deploy/subscriptions/")
        else:
            print(" –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–∞–±–æ—á–∏—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤ —Å —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–º–∏ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞–º–∏")
            # –í—Å—ë —Ä–∞–≤–Ω–æ —Å–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
            test_servers = [
                "vmess://test-vmess",
                "vless://test-vless",
                "ss://test-ss"
            ]
            save_main_files(test_servers, 3)
            split_into_files(test_servers, items_per_file=2)
def generate_readme_table(num_files, base_url="https://raw.githubusercontent.com/hiztin/VLESS-PO-GRIBI/main/deploy/subscriptions"):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–∞–±–ª–∏—Ü—É –¥–ª—è README –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤"""
    
    table = "### üìÅ –†–∞–∑–±–∏–≤–∫–∞ –ø–æ —Ñ–∞–π–ª–∞–º (–ø–æ ~150 —Å–µ—Ä–≤–µ—Ä–æ–≤ –≤ –∫–∞–∂–¥–æ–º)\n\n"
    table += "| –ß–∞—Å—Ç—å | –î–∏–∞–ø–∞–∑–æ–Ω | –°–µ—Ä–≤–µ—Ä–æ–≤ | –°—Å—ã–ª–∫–∞ –¥–ª—è V2Ray (Base64) |\n"
    table += "|-------|----------|----------|---------------------------|\n"
    
    for i in range(1, num_files + 1):
        start = (i-1)*150 + 1
        end = i*150
        table += f"| **{i:02d}** | {start}-{end} | ~150 | [`sub_{i:03d}_b64.txt`]({base_url}/sub_{i:03d}_b64.txt) |\n"
    
    table += f"\n**[üìÇ –°–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ {num_files} —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ subscriptions](https://github.com/hiztin/VLESS-PO-GRIBI/tree/main/deploy/subscriptions)**"
    
    return table

if __name__ == "__main__":
    asyncio.run(main())
